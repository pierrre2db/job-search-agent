# Module Detection - Documentation

Le module **Detection** est responsable du scraping, parsing et scoring des offres d'emploi provenant de multiples sources.

---

## üìã Vue d'ensemble

### Responsabilit√©s

1. **Scraping multi-source** : Indeed, LinkedIn, Pole Emploi, etc.
2. **Parsing intelligent** : Extraction d'informations structur√©es
3. **Scoring** : √âvaluation de la pertinence des offres
4. **Rate limiting** : Respect des limitations des plateformes

### Architecture

```
src/modules/detection/
‚îú‚îÄ‚îÄ __init__.py
‚îú‚îÄ‚îÄ jobboard_scraper.py      # Scrapers pour job boards
‚îú‚îÄ‚îÄ email_parser.py           # Parser d'emails (√† venir)
‚îú‚îÄ‚îÄ scoring_engine.py         # Moteur de scoring (√† venir)
‚îî‚îÄ‚îÄ tests/
    ‚îî‚îÄ‚îÄ test_jobboard_scraper.py
```

---

## üîç Jobboard Scraper

### Classes principales

#### `JobOffer`

Dataclass repr√©sentant une offre d'emploi.

**Attributs :**
```python
@dataclass
class JobOffer:
    title: str                    # Titre du poste
    company: str                  # Nom de l'entreprise
    location: str                 # Localisation
    description: str              # Description (snippet)
    url: str                      # URL de l'offre
    source: str                   # Source (Indeed, LinkedIn, etc.)
    posted_date: Optional[str]    # Date de publication
    salary: Optional[str]         # Fourchette salariale
    contract_type: Optional[str]  # Type de contrat
    remote: bool                  # Travail √† distance
    scraped_at: datetime          # Date du scraping
```

**M√©thodes :**
- `to_dict()` : Convertit l'offre en dictionnaire

#### `BaseJobBoardScraper`

Classe de base pour tous les scrapers.

**Features :**
- User-Agent rotation automatique
- Rate limiting configurable
- Retry automatique avec exponential backoff
- Session HTTP persistante

**Initialisation :**
```python
scraper = BaseJobBoardScraper(
    user_agent="Mozilla/5.0 ...",  # Optionnel
    timeout=30,                     # Timeout des requ√™tes (s)
    max_retries=3,                  # Nombre de tentatives
    rate_limit_delay=(2, 5)         # D√©lai al√©atoire (min, max)
)
```

#### `IndeedScraper`

Scraper sp√©cialis√© pour Indeed.fr

**Utilisation de base :**
```python
from src.modules.detection.jobboard_scraper import IndeedScraper

# Initialiser le scraper
scraper = IndeedScraper()

# Scraper des offres
offers = scraper.scrape(
    query="Python Developer",
    location="Paris",
    max_pages=5,
    radius=25  # Rayon en km
)

# Afficher les r√©sultats
for offer in offers:
    print(f"{offer.title} @ {offer.company}")
    print(f"URL: {offer.url}")
```

**Param√®tres de `scrape()` :**

| Param√®tre | Type | D√©faut | Description |
|-----------|------|--------|-------------|
| `query` | str | *requis* | Mots-cl√©s de recherche |
| `location` | str | "Paris" | Localisation |
| `max_pages` | int | 5 | Nombre de pages √† scraper |
| `radius` | int | 25 | Rayon de recherche (km) |

**M√©thodes avanc√©es :**

```python
# R√©cup√©rer les d√©tails complets d'une offre
details = scraper.get_job_details("https://fr.indeed.com/job/abc123")
print(details['full_description'])
```

---

## üéØ Exemples d'utilisation

### Exemple 1 : Scraping simple

```python
from src.modules.detection.jobboard_scraper import IndeedScraper

scraper = IndeedScraper()

offers = scraper.scrape(
    query="Data Scientist",
    location="Lyon",
    max_pages=3
)

print(f"Trouv√© {len(offers)} offres")

# Filtrer les offres remote
remote_offers = [o for o in offers if o.remote]
print(f"{len(remote_offers)} offres en remote")
```

### Exemple 2 : Scraping avec configuration personnalis√©e

```python
scraper = IndeedScraper(
    timeout=60,
    max_retries=5,
    rate_limit_delay=(3, 7)  # Plus prudent
)

offers = scraper.scrape(
    query="DevOps Engineer",
    location="Remote",
    max_pages=10
)

# Sauvegarder en JSON
import json

offers_data = [o.to_dict() for o in offers]
with open('offers.json', 'w') as f:
    json.dump(offers_data, f, indent=2, ensure_ascii=False)
```

### Exemple 3 : R√©cup√©ration des d√©tails

```python
scraper = IndeedScraper()

# Scraper les offres
offers = scraper.scrape("Full Stack Developer", "Paris", max_pages=1)

# R√©cup√©rer les d√©tails de la premi√®re offre
if offers:
    details = scraper.get_job_details(offers[0].url)
    print(f"Description compl√®te:\n{details['full_description']}")
```

---

## ‚öôÔ∏è Configuration

### Rate Limiting

Le rate limiting est essentiel pour √©viter d'√™tre banni.

**Configuration recommand√©e :**

| Plateforme | D√©lai (s) | Max pages | Notes |
|------------|-----------|-----------|-------|
| Indeed | 2-5 | 10 | Strict sur rate limiting |
| LinkedIn | 3-7 | 5 | Tr√®s strict |
| Pole Emploi | 1-3 | 20 | Plus permissif (API) |

**Personnalisation :**
```python
# Tr√®s prudent (√©viter ban)
scraper = IndeedScraper(rate_limit_delay=(5, 10))

# Rapide (risque de ban)
scraper = IndeedScraper(rate_limit_delay=(0.5, 1))
```

### User-Agent Rotation

Le scraper utilise automatiquement une liste de User-Agents r√©alistes.

**User-Agents inclus :**
- Chrome (Windows, macOS, Linux)
- Firefox (Windows, macOS)
- Safari (macOS)

**Personnalisation :**
```python
scraper = IndeedScraper(
    user_agent="Mozilla/5.0 (Custom) ..."
)
```

---

## üß™ Tests

### Lancer les tests

```bash
# Tous les tests
pytest src/modules/detection/tests/ -v

# Avec couverture
pytest src/modules/detection/tests/ --cov=src.modules.detection

# Tests unitaires uniquement (rapide)
pytest src/modules/detection/tests/ -v -m "not integration"

# Tests d'int√©gration (n√©cessite r√©seau)
pytest src/modules/detection/tests/ -v -m integration
```

### Tests disponibles

| Test | Type | Description |
|------|------|-------------|
| `test_job_offer_creation` | Unit | Cr√©ation d'objet JobOffer |
| `test_parse_search_page` | Unit | Parsing HTML |
| `test_scrape_with_mock` | Unit | Scraping avec mock |
| `test_scrape_pagination` | Unit | Pagination |
| `test_real_indeed_scrape` | Integration | Scraping r√©el (skip par d√©faut) |

---

## üêõ Troubleshooting

### Probl√®me : Aucune offre trouv√©e

**Causes possibles :**
1. S√©lecteurs HTML chang√©s (Indeed modifie r√©guli√®rement sa structure)
2. Blocage IP (trop de requ√™tes)
3. Query trop sp√©cifique

**Solutions :**
```python
# 1. Activer le logging pour debug
import logging
logging.basicConfig(level=logging.DEBUG)

# 2. Augmenter le d√©lai
scraper = IndeedScraper(rate_limit_delay=(5, 10))

# 3. Essayer une query plus g√©n√©rale
offers = scraper.scrape("Developer", "Paris", max_pages=1)
```

### Probl√®me : Erreur 403 Forbidden

**Cause :** Indeed d√©tecte le scraping.

**Solutions :**
1. Augmenter `rate_limit_delay`
2. Utiliser un proxy
3. Attendre quelques heures avant de recommencer

```python
# Avec proxy (√† impl√©menter)
scraper.session.proxies = {
    'http': 'http://proxy.com:8080',
    'https': 'http://proxy.com:8080'
}
```

### Probl√®me : Timeout

**Cause :** Connexion lente ou serveur surcharg√©.

**Solution :**
```python
scraper = IndeedScraper(timeout=60)
```

---

## üìä Performance

### Benchmarks

Tests effectu√©s sur une connexion 100 Mbps :

| Pages | Offres | Temps | Offres/s |
|-------|--------|-------|----------|
| 1 | 10 | 5s | 2.0 |
| 5 | 50 | 35s | 1.4 |
| 10 | 100 | 80s | 1.25 |

**Note :** Le temps inclut le rate limiting (2-5s entre pages).

### Optimisations possibles

1. **Scraping parall√®le** : Utiliser asyncio pour scraper plusieurs pages en parall√®le
2. **Caching** : Mettre en cache les r√©sultats pour √©viter de re-scraper
3. **Proxies** : Utiliser un pool de proxies pour augmenter le d√©bit

---

## üîú Prochaines √©volutions

- [ ] Support de LinkedIn scraping
- [ ] Support de Pole Emploi API
- [ ] Support de Welcome to the Jungle
- [ ] Scraping asynchrone (asyncio)
- [ ] Pool de proxies automatique
- [ ] D√©tection automatique de changements HTML
- [ ] Export vers base de donn√©es

---

## üìö Ressources

- [BeautifulSoup Documentation](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)
- [Requests Documentation](https://requests.readthedocs.io/)
- [Tenacity (Retry) Documentation](https://tenacity.readthedocs.io/)

---
